{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook explains a basic workflow two tables using *py_entitymatching*. Our goal is to come up with a workflow to match DBLP and ACM datasets. Specifically, we want to achieve precision greater than 95% and get recall greater than 90%. The datasets contain information about the conference papers published in top databse conferences.\n",
    "\n",
    "First, we need to import *py_entitymatching* package and other libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/pradap/Documents/Research/Python-Package/anhaid/py_entitymatching/')\n",
    "\n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 2.7.13 | packaged by conda-forge | (default, May  2 2017, 13:29:36) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]\n",
      "pandas version: 0.20.3\n",
      "magellan version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "# Display the versions\n",
    "print('python version: ' + sys.version )\n",
    "print('pandas version: ' + pd.__version__ )\n",
    "print('magellan version: ' + em.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching two tables typically consists of the following three steps:\n",
    "\n",
    "** 1. Reading the input tables **\n",
    "\n",
    "** 2. Blocking the input tables to get a candidate set **\n",
    "\n",
    "** 3. Matching the tuple pairs in the candidate set **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the input tables. For the purpose of this guide, we use the datasets that are included with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths\n",
    "path_A = em.get_install_path() + os.sep + 'datasets' + os.sep + 'end-to-end' + os.sep + 'dblp_demo.csv'\n",
    "path_B = em.get_install_path() + os.sep + 'datasets' + os.sep + 'end-to-end' + os.sep + 'acm_demo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load csv files as dataframes and set the key attribute in the dataframe\n",
    "A = em.read_csv_metadata(path_A, key='id')\n",
    "B = em.read_csv_metadata(path_B, key='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuples in A: 1800\n",
      "Number of tuples in B: 1813\n",
      "Number of tuples in A X B (i.e the cartesian product): 3263400\n"
     ]
    }
   ],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l0</td>\n",
       "      <td>Paradise: A Database System for GIS Applications</td>\n",
       "      <td>Paradise Team</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l1</td>\n",
       "      <td>A Query Language and Optimization Techniques for Unstructured Data</td>\n",
       "      <td>Gerd G. Hillebrand, Peter Buneman, Susan B. Davidson, Dan Suciu</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                               title  \\\n",
       "0  l0                    Paradise: A Database System for GIS Applications   \n",
       "1  l1  A Query Language and Optimization Techniques for Unstructured Data   \n",
       "\n",
       "                                                           authors  \\\n",
       "0                                                    Paradise Team   \n",
       "1  Gerd G. Hillebrand, Peter Buneman, Susan B. Davidson, Dan Suciu   \n",
       "\n",
       "               venue  paper year  \n",
       "0  SIGMOD Conference        1995  \n",
       "1  SIGMOD Conference        1996  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r0</td>\n",
       "      <td>An efficient bitmap encoding scheme for selection queries</td>\n",
       "      <td>Chee-Yong Chan, Yannis E. Ioannidis</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1</td>\n",
       "      <td>Integrating a Structured-Text Retrieval System with an Object-Oriented Database System</td>\n",
       "      <td>Tak W. Yan, Jurgen Annevelink</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0  r0   \n",
       "1  r1   \n",
       "\n",
       "                                                                                    title  \\\n",
       "0                               An efficient bitmap encoding scheme for selection queries   \n",
       "1  Integrating a Structured-Text Retrieval System with an Object-Oriented Database System   \n",
       "\n",
       "                               authors  \\\n",
       "0  Chee-Yong Chan, Yannis E. Ioannidis   \n",
       "1        Tak W. Yan, Jurgen Annevelink   \n",
       "\n",
       "                                            venue  paper year  \n",
       "0  International Conference on Management of Data        1999  \n",
       "1                           Very Large Data Bases        1994  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('id', 'id')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the key attributes of table A and B.\n",
    "em.get_key(A), em.get_key(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Tables To Get Candidate Set\n",
    "\n",
    "Before we do the matching, we would like to remove the obviously non-matching tuple pairs from the input tables. This would reduce the number of tuple pairs considered for matching.\n",
    "*py_entitymatching* provides four different blockers: (1) attribute equivalence, (2) overlap, (3) rule-based, and (4) black-box. The user can mix and match these blockers to form a blocking sequence applied to input tables.\n",
    "\n",
    "For the matching problem at hand, we know that two conference papers published in different years cannot match, or if there are errors in the year then there should be at least some overlap between the paper titles. So we decide the apply the following blocking plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Blocking plan\n",
    "\n",
    "# A, B -- AttrEquivalence blocker [year] --------------------|\n",
    "#                                                           |---> candidate set\n",
    "# A, B -- Overlap blocker [title]---------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327703"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create attribute equivalence blocker\n",
    "ab = em.AttrEquivalenceBlocker()\n",
    "# Block tables using 'year' attribute : same year include in candidate set\n",
    "C1 = ab.block_tables(A, B, 'paper year', 'paper year', \n",
    "                     l_output_attrs=['title', 'authors', 'paper year'],\n",
    "                     r_output_attrs=['title', 'authors', 'paper year']\n",
    "                    )\n",
    "len(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244220"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "# Block over title attribute\n",
    "C2 = ob.block_tables(A, B, 'title', 'title', show_progress=False, overlap_size=2)\n",
    "len(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544632"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the outputs from attr. equivalence blocker and overlap blocker\n",
    "C = em.combine_blocker_outputs_via_union([C1, C2])\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Tuple Pairs in Candidate Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we would want to match the tuple pairs in the candidate set. Specifically, we use learning-based method for matching purposes.\n",
    "This typically involves the following four steps:\n",
    "\n",
    "1. Sampling and labeling the candidate set\n",
    "2. Splitting the labeled data into development and evaluation set\n",
    "3. Selecting the best learning based matcher using the development set\n",
    "4. Evaluating the selected matcher using the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and labeling the candidate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we randomly sample 450 tuple pairs for labeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample  candidate set\n",
    "S = em.sample_table(C, 450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we label the sampled candidate set. Specify we would enter 1 for a match and 0 for a non-match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label S\n",
    "#G = em.label_table(S, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this guide, we will load in a pre-labeled dataset (of 450 tuple pairs) included in this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"py_entitymatching.io.parsers\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-labeled data\n",
    "path_G = em.get_install_path() + os.sep + 'datasets' + os.sep + 'end-to-end' + os.sep + 'labeled_data_demo.csv'\n",
    "G = em.read_csv_metadata(path_G, \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the labeled data into development and evaluation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we split the labeled data into two sets: development (I) and evaluation (J). Specifically, the development set is used to come up with the best learning-based matcher and the evaluation set used to evaluate the selected matcher on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split S into development set (I) and evaluation set (J)\n",
    "IJ = em.split_train_test(G, train_proportion=0.7, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best learning-based matcher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best learning-based matcher typically involves the following steps:\n",
    "\n",
    "1. Creating a set of learning-based matchers\n",
    "2. Creating features\n",
    "3. Converting the development set into feature vectors\n",
    "4. Selecting the best learning-based matcher using k-fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Set of Learning-based Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a set of features for the development set. *py_entitymatching* provides a way to automatically generate features based on the attributes in the input tables. For the purposes of this guide, we use the automatically generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "feature_table = em.get_features_for_matching(A, B, validate_inferred_attr_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          id_id_lev_dist\n",
       "1                           id_id_lev_sim\n",
       "2                               id_id_jar\n",
       "3                               id_id_jwn\n",
       "4                               id_id_exm\n",
       "5                   id_id_jac_qgm_3_qgm_3\n",
       "6             title_title_jac_qgm_3_qgm_3\n",
       "7         title_title_cos_dlm_dc0_dlm_dc0\n",
       "8                         title_title_mel\n",
       "9                    title_title_lev_dist\n",
       "10                    title_title_lev_sim\n",
       "11        authors_authors_jac_qgm_3_qgm_3\n",
       "12    authors_authors_cos_dlm_dc0_dlm_dc0\n",
       "13                    authors_authors_mel\n",
       "14               authors_authors_lev_dist\n",
       "15                authors_authors_lev_sim\n",
       "16              paper_year_paper_year_exm\n",
       "17              paper_year_paper_year_anm\n",
       "18         paper_year_paper_year_lev_dist\n",
       "19          paper_year_paper_year_lev_sim\n",
       "Name: feature_name, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the names of the features generated\n",
    "feature_table['feature_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Development Set to  Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=feature_table, \n",
    "                            attrs_after='label',\n",
    "                            show_progress=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>id_id_lev_dist</th>\n",
       "      <th>id_id_lev_sim</th>\n",
       "      <th>id_id_jar</th>\n",
       "      <th>id_id_jwn</th>\n",
       "      <th>id_id_exm</th>\n",
       "      <th>id_id_jac_qgm_3_qgm_3</th>\n",
       "      <th>title_title_jac_qgm_3_qgm_3</th>\n",
       "      <th>...</th>\n",
       "      <th>authors_authors_jac_qgm_3_qgm_3</th>\n",
       "      <th>authors_authors_cos_dlm_dc0_dlm_dc0</th>\n",
       "      <th>authors_authors_mel</th>\n",
       "      <th>authors_authors_lev_dist</th>\n",
       "      <th>authors_authors_lev_sim</th>\n",
       "      <th>paper_year_paper_year_exm</th>\n",
       "      <th>paper_year_paper_year_anm</th>\n",
       "      <th>paper_year_paper_year_lev_dist</th>\n",
       "      <th>paper_year_paper_year_lev_sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>l1114</td>\n",
       "      <td>r597</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548358</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>439</td>\n",
       "      <td>l1577</td>\n",
       "      <td>r688</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536692</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>l1174</td>\n",
       "      <td>r588</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515819</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _id ltable_id rtable_id  id_id_lev_dist  id_id_lev_sim  id_id_jar  \\\n",
       "221  221     l1114      r597               5            0.0        0.0   \n",
       "439  439     l1577      r688               5            0.0        0.0   \n",
       "191  191     l1174      r588               5            0.0        0.0   \n",
       "\n",
       "     id_id_jwn  id_id_exm  id_id_jac_qgm_3_qgm_3  title_title_jac_qgm_3_qgm_3  \\\n",
       "221        0.0          0                    0.0                     0.057692   \n",
       "439        0.0          0                    0.0                     0.052980   \n",
       "191        0.0          0                    0.0                     0.072464   \n",
       "\n",
       "     ...    authors_authors_jac_qgm_3_qgm_3  \\\n",
       "221  ...                           0.035294   \n",
       "439  ...                           0.088235   \n",
       "191  ...                           0.044444   \n",
       "\n",
       "     authors_authors_cos_dlm_dc0_dlm_dc0  authors_authors_mel  \\\n",
       "221                                  0.0             0.548358   \n",
       "439                                  0.0             0.536692   \n",
       "191                                  0.0             0.515819   \n",
       "\n",
       "     authors_authors_lev_dist  authors_authors_lev_sim  \\\n",
       "221                      41.0                 0.226415   \n",
       "439                      61.0                 0.140845   \n",
       "191                      45.0                 0.166667   \n",
       "\n",
       "     paper_year_paper_year_exm  paper_year_paper_year_anm  \\\n",
       "221                          1                        1.0   \n",
       "439                          1                        1.0   \n",
       "191                          1                        1.0   \n",
       "\n",
       "     paper_year_paper_year_lev_dist  paper_year_paper_year_lev_sim  label  \n",
       "221                             0.0                            1.0      0  \n",
       "439                             0.0                            1.0      0  \n",
       "191                             0.0                            1.0      0  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "H.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Matcher Using Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select the best matcher using k-fold cross-validation. For the purposes of this guide, we use five fold cross validation and use 'precision' and 'recall' metric to select the best matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Average recall</th>\n",
       "      <th>Average f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.905960</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.909402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.957971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842432</td>\n",
       "      <td>0.911244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.951622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.952866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Matcher  Average precision  Average recall  Average f1\n",
       "0  DecisionTree           0.905960        0.922281    0.909402\n",
       "1            RF           1.000000        0.922281    0.957971\n",
       "2           SVM           1.000000        0.842432    0.911244\n",
       "3        LinReg           1.000000        0.912281    0.951622\n",
       "4        LogReg           0.988889        0.922281    0.952866"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'],\n",
    "        k=5,\n",
    "        target_attr='label', metric_to_select_matcher='precision', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Average recall</th>\n",
       "      <th>Average f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.905960</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.909402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.957971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842432</td>\n",
       "      <td>0.911244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.951622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.922281</td>\n",
       "      <td>0.952866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Matcher  Average precision  Average recall  Average f1\n",
       "0  DecisionTree           0.905960        0.922281    0.909402\n",
       "1            RF           1.000000        0.922281    0.957971\n",
       "2           SVM           1.000000        0.842432    0.911244\n",
       "3        LinReg           1.000000        0.912281    0.951622\n",
       "4        LogReg           0.988889        0.922281    0.952866"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'],\n",
    "        k=5,\n",
    "        target_attr='label', metric_to_select_matcher='recall', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the best matcher (RF) is  getting us to the precision and recall that we expect (i.e P > 95% and R > 90%). So, we select this matcher and now we can proceed on to evaluating the best matcher on the unseen data (the evaluation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating the Matching Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the matching outputs for the evaluation set typically involves the following four steps:\n",
    "1. Converting the evaluation set to feature vectors\n",
    "2. Training matcher using the feature vectors extracted from the development set\n",
    "3. Predicting the evaluation set using the trained matcher\n",
    "4. Evaluating the predicted matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Evaluation Set to  Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we convert to the feature vectors (using the feature table and the evaluation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert J into a set of feature vectors using feature table\n",
    "L = em.extract_feature_vecs(J, feature_table=feature_table,\n",
    "                            attrs_after='label', show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Selected Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the matcher using all of the feature vectors from the development set. For the purposes of this guide we use random forest as the selected matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train using feature vectors from I \n",
    "dt.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'], \n",
    "       target_attr='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we predict the matches for the evaluation set (using the feature vectors extracted from it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on L \n",
    "predictions = dt.predict(table=L, exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'], \n",
    "              append=True, target_attr='predicted', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Matching Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the accuracy of predicted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 95.12% (39/41)\n",
      "Recall : 97.5% (39/40)\n",
      "F1 : 96.3%\n",
      "False positives : 2 (out of 41 positive predictions)\n",
      "False negatives : 1 (out of 94 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
